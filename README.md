# ğŸ›’ Online Sales Data Pipeline

## ğŸ‡ºğŸ‡¸ English Version

### ğŸ“˜ Project Overview
This project simulates a complete **data engineering pipeline** for an **e-commerce platform**, covering data extraction, transformation, loading (ETL), and analysis.  
The goal is to build a robust and scalable pipeline capable of transforming raw sales data into valuable business insights.

---

### ğŸš€ Architecture

**Components:**
- **Python:** used for data extraction, transformation, and loading (ETL scripts)  
- **Airflow:** orchestration of ETL tasks  
- **Snowflake:** cloud data warehouse for structured storage and analytics  
- **Docker:** containerized environment for local development and deployment  
- **Bitbucket / GitHub:** version control and CI/CD integration  
- **Power BI / Tableau (optional):** for dashboards and data visualization  

---

### ğŸ“ Data Flow

1. **Extraction:** Data collected from CSV files, APIs, or Kaggle datasets via Kaggle API.  
2. **Transformation:** Data cleaning, normalization, and enrichment using Python and SQL.  
3. **Loading:** Processed data stored in Snowflake, partitioned by date and category.  
4. **Analytics:** Creation of summary tables and visual dashboards.

---

### ğŸ“Œ Data Extraction via Kaggle API

To run the extraction script, the user must:

1. Create a Kaggle account.  
2. Generate an API token (`kaggle.json`) in your Kaggle account settings.  
3. Place `kaggle.json` in the appropriate location:  
   - `~/.kaggle/` on Linux/macOS  
   - `C:\Users\<user>\.kaggle\` on Windows  
4. Install the Kaggle Python package:

```bash
pip install kaggle
```

5. Run the extraction script:

```bash
python scripts/extract_kaggle.py
```

> The script downloads the dataset and saves it in `data/raw/`.  
> **Important:** Do not commit `kaggle.json` to GitHub.

After extraction, processed and cleaned data is saved in `data/processed/` for further analysis.

---

### ğŸ“Š Example Insights

- Total revenue by product category  
- Top 10 best-selling products  
- Average order value per customer segment  
- Monthly sales growth rate  

---

### âš™ï¸ Technologies Used

- Python 3.10+  
- Snowflake  
- Apache Airflow  
- Docker  
- Bitbucket / GitHub  
- Pandas, SQLAlchemy, Requests  

---

### ğŸ§± Repository Structure
```
online-sales-pipeline/
â”‚
â”œâ”€â”€ dags/              # Airflow DAGs
â”œâ”€â”€ scripts/           # Python ETL scripts
â”œâ”€â”€ data/              # Input data
â”‚   â”œâ”€â”€ raw/           # Raw datasets (from Kaggle/API)
â”‚   â””â”€â”€ processed/     # Cleaned and transformed data
â”œâ”€â”€ sql/               # SQL queries and transformations
â”œâ”€â”€ notebooks/         # Exploratory analysis
â”œâ”€â”€ docker/            # Docker configuration files
â”œâ”€â”€ README.md          # Project documentation
â””â”€â”€ requirements.txt   # Dependencies
```

---

### ğŸ’¡ Goal

To demonstrate end-to-end **data engineering skills** â€” from ingestion to visualization â€” using real-world tools and best practices.

---

## ğŸ‡·ğŸ‡· VersÃ£o em PortuguÃªs

### ğŸ“˜ VisÃ£o Geral do Projeto
Este projeto simula um **pipeline completo de engenharia de dados** para uma **plataforma de e-commerce**, cobrindo as etapas de extraÃ§Ã£o, transformaÃ§Ã£o, carga (ETL) e anÃ¡lise.  
O objetivo Ã© construir um pipeline robusto e escalÃ¡vel que transforme dados brutos de vendas em **insights de negÃ³cio valiosos**.

---

### ğŸš€ Arquitetura

**Componentes:**
- **Python:** extraÃ§Ã£o, transformaÃ§Ã£o e carga de dados (ETL)  
- **Airflow:** orquestraÃ§Ã£o das tarefas  
- **Snowflake:** armazenamento e anÃ¡lise dos dados estruturados  
- **Docker:** ambiente containerizado para desenvolvimento e deploy  
- **Bitbucket / GitHub:** controle de versÃ£o e CI/CD  
- **Power BI / Tableau (opcional):** visualizaÃ§Ã£o dos dados  

---

### ğŸ—‚ï¸ Fluxo de Dados

1. **ExtraÃ§Ã£o:** dados coletados de arquivos CSV, APIs ou datasets do Kaggle via API.  
2. **TransformaÃ§Ã£o:** limpeza, normalizaÃ§Ã£o e enriquecimento dos dados usando Python e SQL.  
3. **Carga:** dados processados armazenados no Snowflake, particionados por data e categoria.  
4. **AnÃ¡lise:** criaÃ§Ã£o de tabelas resumo e dashboards visuais.

---

### ğŸ“Œ ExtraÃ§Ã£o de Dados via API do Kaggle

Para rodar o script de extraÃ§Ã£o, o usuÃ¡rio deve:

1. Criar uma conta no Kaggle.  
2. Gerar um token de API (`kaggle.json`) nas configuraÃ§Ãµes da conta.  
3. Colocar `kaggle.json` no local correto:  
   - `~/.kaggle/` no Linux/macOS  
   - `C:\Users\<usuÃ¡rio>\.kaggle\` no Windows  
4. Instalar o pacote Python do Kaggle:

```bash
pip install kaggle
```

5. Rodar o script de extraÃ§Ã£o:

```bash
python scripts/extract_kaggle.py
```

> O script baixa o dataset e salva em `data/raw/`.  
> **Importante:** nÃ£o envie `kaggle.json` para o GitHub.

ApÃ³s a extraÃ§Ã£o, os dados processados e limpos sÃ£o salvos em `data/processed/` para anÃ¡lise posterior.

---

### ğŸ“Š Exemplos de Insights

- Receita total por categoria de produto  
- Top 10 produtos mais vendidos  
- Ticket mÃ©dio por segmento de cliente  
- Crescimento mensal de vendas  

---

### âš™ï¸ Tecnologias Utilizadas

- Python 3.10+  
- Snowflake  
- Apache Airflow  
- Docker  
- Bitbucket / GitHub  
- Pandas, SQLAlchemy, Requests  

---

### ğŸ§± Estrutura do RepositÃ³rio
```
online-sales-pipeline/
â”‚
â”œâ”€â”€ dags/               # DAGs do Airflow
â”œâ”€â”€ scripts/            # Scripts ETL em Python
â”œâ”€â”€ data/              # Dados de entrada
â”‚   â”œâ”€â”€ raw/           # Datasets brutos (do Kaggle/API)
â”‚   â””â”€â”€ processed/     # Dados limpos e transformados
â”œâ”€â”€ sql/               # Consultas e transformaÃ§Ãµes SQL
â”œâ”€â”€ notebooks/         # AnÃ¡lises exploratÃ³rias
â”œâ”€â”€ docker/            # Arquivos de configuraÃ§Ã£o do Docker
â”œâ”€â”€ README.md          # DocumentaÃ§Ã£o do projeto
â””â”€â”€ requirements.txt   # DependÃªncias
```

---

### ğŸ’¡ Objetivo

Demonstrar **habilidades completas em engenharia de dados** â€” da ingestÃ£o Ã  visualizaÃ§Ã£o â€” usando ferramentas reais e boas prÃ¡ticas do mercado.

